{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Q4"
      ],
      "metadata": {
        "id": "1KaMcY5tqtvo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Understanding Word Order in Transformers via Positional Encoding\n",
        "\n",
        "Transformers, by their core self-attention mechanism, do not inherently process words in sequence. To understand word order, they utilize **Positional Encoding (PE)**.\n",
        "\n",
        "1.  **Injecting Position Information:** PE adds a vector representing a word's position in the sequence to its semantic word embedding (E). This results in a combined vector $X = E + PE$ for each word. Each $X$ vector thus carries information about both the word's meaning and its absolute position.\n",
        "\n",
        "2.  **Attention Mechanism with Positional Awareness:** The self-attention mechanism then processes these position-aware $X$ vectors. For each word $i$, its $X_i$ vector is transformed into Query ($Q_i$), Key ($K_i$), and Value ($V_i$) vectors by multiplying with learned weight matrices ($W_Q, W_K, W_V$).\n",
        "\n",
        "    $Q_i = X_i \\cdot W_Q$\n",
        "    $K_i = X_i \\cdot W_K$\n",
        "    $V_i = X_i \\cdot W_V$\n",
        "\n",
        "3.  **Calculating Attention Scores:** The attention score from word $i$ to word $j$ is typically the dot product $Score(i,j) = Q_i \\cdot K_j^T$ (often followed by scaling and softmax). Since $Q_i$ and $K_j$ are derived from $X_i$ and $X_j$ (which include distinct positional information), these scores naturally reflect the words' relative positions.\n",
        "\n",
        "4.  **Numeric Illustration:**\n",
        "    Consider two words with their $X = E+PE$ vectors (simplified to 2D):\n",
        "    * Word 1 (\"I\" at pos 0): $X_1 = [0.10, 1.30]$\n",
        "    * Word 2 (\"really\" at pos 1): $X_2 = [1.74, 0.64]$\n",
        "\n",
        "    Using illustrative learned weight matrices\n",
        "    \n",
        "    $W_Q = \\begin{pmatrix} 1 & 2 \\\\ 3 & 1 \\end{pmatrix}$ and $W_K = \\begin{pmatrix} 2 & 1 \\\\ 1 & 3 \\end{pmatrix}$, we derive:\n",
        "    * For Word 1: $Q_1 = X_1 \\cdot W_Q = [4.00, 1.50]$ and $K_1 = X_1 \\cdot W_K = [1.50, 4.00]$\n",
        "    * For Word 2: $K_2 = X_2 \\cdot W_K = [4.12, 3.66]$ (calculation: $Q_2$ and $V_2$ also derived similarly)\n",
        "\n",
        "    Now, let's see how Word 1 (\"I\") attends to other words:\n",
        "    * Score(\"I\" $\\rightarrow$ \"I\"): $Score(1,1) = Q_1 \\cdot K_1^T = [4.00, 1.50] \\cdot [1.50, 4.00]^T = (4.00 \\times 1.50) + (1.50 \\times 4.00) = 12.00$\n",
        "    * Score(\"I\" $\\rightarrow$ \"really\"): $Score(1,2) = Q_1 \\cdot K_2^T = [4.00, 1.50] \\cdot [4.12, 3.66]^T = (4.00 \\times 4.12) + (1.50 \\times 3.66) = 16.48 + 5.49 = 21.97$\n",
        "\n",
        "    These different scores (12.00 vs. 21.97) demonstrate that Word 1 attends differently to itself versus Word 2. This differentiation is possible because $K_1$ and $K_2$ are distinct, influenced by the unique PEs within $X_1$ and $X_2$.\n",
        "\n",
        "5.  **Learning Order-Dependent Relationships:** The crucial aspect is that the weight matrices ($W_Q, W_K, W_V$) are *learned* during training. The model adjusts these weights to optimize its performance on language tasks. By doing so, it learns to interpret the patterns introduced by PEs in the $X$ vectors, allowing the attention mechanism to effectively consider word order and model sequential dependencies. For instance, it can learn that a query vector $Q_i$ (from $X_i$ with $PE_i$) should pay more or less attention to a key vector $K_j$ (from $X_j$ with $PE_j$) based on the relationship between positions $i$ and $j$ and the words' semantics.\n"
      ],
      "metadata": {
        "id": "dm957KVkFJPQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Detailed Explanation"
      ],
      "metadata": {
        "id": "xHJ3MtUNrhLc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The core of a transformer, the self-attention mechanism, doesn't inherently know the order of words. If you just fed it word embeddings, it would treat \"I enjoy learning transformers\" the same as \"Transformers learning enjoy I\" – it would see the same set of words, just jumbled. This is where **positional encoding (PE)** comes in.\n",
        "\n",
        "---\n",
        "## The Role of Positional Encoding\n",
        "\n",
        "Here's the step-by-step breakdown:\n",
        "\n",
        "### 1. Word Embeddings (E)\n",
        "First, each word in your input sentence is converted into a numerical vector called a **word embedding**. This embedding captures the *meaning* of the word. For example:\n",
        "* \"I\" -> `[some_vector_representing_I]`\n",
        "* \"really\" -> `[some_vector_representing_really]`\n",
        "* ...and so on.\n",
        "\n",
        "These embeddings by themselves don't contain any information about the word's position in the sentence.\n",
        "\n",
        "### 2. Positional Encoding (PE)\n",
        "To give the model a sense of order, we create another vector for each position in the sequence – the **positional encoding**. This PE vector is designed so that:\n",
        "* Each position (e.g., 1st word, 2nd word, 3rd word) gets a unique encoding.\n",
        "* The encodings for different positions are distinct.\n",
        "* The model can learn to interpret these encodings to understand relative positions (e.g., \"this word comes before that word,\" or \"these two words are N positions apart\").\n",
        "\n",
        "Commonly, these positional encodings are generated using **sinusoidal functions** (sine and cosine functions of different frequencies and offsets). This mathematical approach has a couple of nice properties:\n",
        "* It can generate unique encodings for sequences longer than those seen during training.\n",
        "* It allows the model to easily learn relative positions because the difference or relationship between PE vectors for different positions can be consistently represented.\n",
        "\n",
        "### 3. Combining Embeddings and Positional Encodings (X = E + PE)\n",
        "This is where your example comes in. The word embedding (E) for each word is **added** to its corresponding positional encoding (PE).\n",
        "\n",
        "So, for your sentence:\n",
        "* \"I\" (at position 0)\n",
        "* \"really\" (at position 1)\n",
        "* \"enjoy\" (at position 2)\n",
        "* \"learning\" (at position 3)\n",
        "* \"transformers\" (at position 4)\n",
        "\n",
        "The model gets:\n",
        "* `X_I` = `Embedding(\"I\")` + `PE(position_0)`\n",
        "* `X_really` = `Embedding(\"really\")` + `PE(position_1)`\n",
        "* `X_enjoy` = `Embedding(\"enjoy\")` + `PE(position_2)`\n",
        "* `X_learning` = `Embedding(\"learning\")` + `PE(position_3)`\n",
        "* `X_transformers` = `Embedding(\"transformers\")` + `PE(position_4)`\n",
        "\n",
        "Your provided output represents these combined vectors `X`:\n",
        "```\n",
        "# Word      E + PE (X)\n",
        "# I         0.10  1.30  0.40  1.80  (This vector now implicitly carries info that \"I\" is at the start)\n",
        "# really    1.74  0.64  0.51  1.19  (This vector knows \"really\" is the second word)\n",
        "# enjoy     1.60  0.18  0.12  1.39  (And so on...)\n",
        "# learning  0.34 -0.18  0.32  1.69\n",
        "# transforme -0.25 -0.25 0.93  1.29\n",
        "```\n",
        "Crucially, even if two words in different positions had *identical* word embeddings (which is unlikely but possible for homonyms if context isn't perfectly captured yet), their combined `E+PE` vectors would still be different because their `PE` components would be different due to their different positions.\n",
        "\n",
        "### 4. How the Model \"Understands\" Order\n",
        "Now that each word's input representation (`X`) subtly includes information about its original position, the **self-attention mechanisms** in the transformer can learn to use this.\n",
        "\n",
        "When the self-attention mechanism calculates how much attention a word should pay to other words in the sequence, it's doing so based on these combined `E+PE` vectors.\n",
        "* The model learns patterns. For example, it might learn that a word with a PE indicating it's at the beginning of a sentence often has a strong grammatical relationship with a word whose PE indicates it's a verb later in the sentence.\n",
        "* The unique \"signature\" provided by the PE allows the attention mechanism to distinguish between \"the cat chased the dog\" and \"the dog chased the cat\". The word embeddings for \"cat\" and \"dog\" are the same in both sentences, but their associated positional encodings will differ, leading to different attention patterns and ultimately different interpretations.\n",
        "\n",
        "So, the model doesn't \"read\" the positional encoding like a human reads a number. Instead, the **patterns and relative differences** in the PE values, when combined with the word embeddings, allow the network's learned weights (especially in the attention layers) to **implicitly model the sequence order** and make sense of grammatical structure and dependencies that rely on word order. The model learns that words with certain types of positional encodings (e.g., earlier ones) tend to interact with words with other types of positional encodings (e.g., later ones) in predictable ways."
      ],
      "metadata": {
        "id": "lDPHjm8EE5Va"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "let's delve into a numeric example to illustrate how the attention mechanism, using the `X` matrix (Input Embedding + Positional Encoding), can learn the order of words.\n",
        "\n",
        "For simplicity, we'll:\n",
        "1.  Use only two words from your example.\n",
        "2.  Reduce the dimensionality of their `X` vectors to 2D.\n",
        "3.  Use very simple, illustrative weight matrices (`W_Q`, `W_K`, `W_V`). In a real transformer, these are learned and much larger.\n",
        "4.  Focus on the calculation of attention scores.\n",
        "\n",
        "### 1. Simplified Input Vectors (X)\n",
        "\n",
        "Let's take the first two words from your example and simplify their `E+PE` vectors (which we call `X`) to 2 dimensions:\n",
        "* Word 1 (\"I\") at position 0: `X_1 = [0.10, 1.30]`\n",
        "* Word 2 (\"really\") at position 1: `X_2 = [1.74, 0.64]`\n",
        "\n",
        "These vectors `X_1` and `X_2` are different due to both the words themselves (\"I\" vs. \"really\") and their positions (0 vs. 1), which are captured by the Positional Encoding (PE) component that was added to the original word embeddings.\n",
        "\n",
        "### 2. Weight Matrices (WQ, WK, WV)\n",
        "\n",
        "The transformer learns three weight matrices for each attention head:\n",
        "* $W_Q$ (for Query)\n",
        "* $W_K$ (for Key)\n",
        "* $W_V$ (for Value)\n",
        "\n",
        "These matrices are multiplied by the input vectors `X` to get the Query (`Q`), Key (`K`), and Value (`V`) vectors for each word. The values in these weight matrices are learned during the training process.\n",
        "\n",
        "For our example, let's define very simple 2x2 weight matrices:\n",
        "Let $W_Q = \\begin{pmatrix} 1 & 2 \\\\ 3 & 1 \\end{pmatrix}$\n",
        "Let $W_K = \\begin{pmatrix} 2 & 1 \\\\ 1 & 3 \\end{pmatrix}$\n",
        "Let $W_V = \\begin{pmatrix} 0.5 & 1.5 \\\\ 1 & 0.5 \\end{pmatrix}$\n",
        "\n",
        "### 3. Calculating Q, K, and V Vectors\n",
        "\n",
        "For each input vector `X_i`, we calculate `Q_i`, `K_i`, and `V_i`:\n",
        "$Q_i = X_i \\cdot W_Q$\n",
        "$K_i = X_i \\cdot W_K$\n",
        "$V_i = X_i \\cdot W_V$\n",
        "\n",
        "**For Word 1 (\"I\"):** $X_1 = [0.10, 1.30]$\n",
        "$Q_1 = [0.10, 1.30] \\cdot \\begin{pmatrix} 1 & 2 \\\\ 3 & 1 \\end{pmatrix} = [(0.10*1 + 1.30*3), (0.10*2 + 1.30*1)] = [0.10 + 3.90, 0.20 + 1.30] = [4.00, 1.50]$\n",
        "$K_1 = [0.10, 1.30] \\cdot \\begin{pmatrix} 2 & 1 \\\\ 1 & 3 \\end{pmatrix} = [(0.10*2 + 1.30*1), (0.10*1 + 1.30*3)] = [0.20 + 1.30, 0.10 + 3.90] = [1.50, 4.00]$\n",
        "$V_1 = [0.10, 1.30] \\cdot \\begin{pmatrix} 0.5 & 1.5 \\\\ 1 & 0.5 \\end{pmatrix} = [(0.10*0.5 + 1.30*1), (0.10*1.5 + 1.30*0.5)] = [0.05 + 1.30, 0.15 + 0.65] = [1.35, 0.80]$\n",
        "\n",
        "**For Word 2 (\"really\"):** $X_2 = [1.74, 0.64]$\n",
        "$Q_2 = [1.74, 0.64] \\cdot \\begin{pmatrix} 1 & 2 \\\\ 3 & 1 \\end{pmatrix} = [(1.74*1 + 0.64*3), (1.74*2 + 0.64*1)] = [1.74 + 1.92, 3.48 + 0.64] = [3.66, 4.12]$\n",
        "$K_2 = [1.74, 0.64] \\cdot \\begin{pmatrix} 2 & 1 \\\\ 1 & 3 \\end{pmatrix} = [(1.74*2 + 0.64*1), (1.74*1 + 0.64*3)] = [3.48 + 0.64, 1.74 + 1.92] = [4.12, 3.66]$\n",
        "$V_2 = [1.74, 0.64] \\cdot \\begin{pmatrix} 0.5 & 1.5 \\\\ 1 & 0.5 \\end{pmatrix} = [(1.74*0.5 + 0.64*1), (1.74*1.5 + 0.64*0.5)] = [0.87 + 0.64, 2.61 + 0.32] = [1.51, 2.93]$\n",
        "\n",
        "### 4. Calculating Attention Scores\n",
        "\n",
        "The attention score between a query word `i` and a key word `j` is calculated as $Score(i,j) = Q_i \\cdot K_j^T$. This is then typically scaled by $\\sqrt{d_k}$ (dimension of key vectors), but we'll ignore scaling for this raw score illustration.\n",
        "\n",
        "**Attention scores from Word 1 (\"I\")'s perspective (using $Q_1$):**\n",
        "* Score of \"I\" attending to \"I\":\n",
        "    $Score(1,1) = Q_1 \\cdot K_1^T = [4.00, 1.50] \\cdot [1.50, 4.00]^T = (4.00 * 1.50) + (1.50 * 4.00) = 6.00 + 6.00 = 12.00$\n",
        "* Score of \"I\" attending to \"really\":\n",
        "    $Score(1,2) = Q_1 \\cdot K_2^T = [4.00, 1.50] \\cdot [4.12, 3.66]^T = (4.00 * 4.12) + (1.50 * 3.66) = 16.48 + 5.49 = 21.97$\n",
        "\n",
        "These raw scores ($12.00, 21.97$) would then go through a Softmax function to get attention weights:\n",
        "$Weights_1 = \\text{Softmax}([12.00, 21.97])$\n",
        "$e^{12.00} \\approx 162755$\n",
        "$e^{21.97} \\approx 3.46 \\times 10^9$\n",
        "Sum $\\approx 3.46 \\times 10^9$\n",
        "$Weight(I \\rightarrow I) = 162755 / (3.46 \\times 10^9) \\approx 0.000047$\n",
        "$Weight(I \\rightarrow \\text{really}) = (3.46 \\times 10^9) / (3.46 \\times 10^9) \\approx 0.999953$\n",
        "So, for \"I\", it pays overwhelmingly more attention to \"really\".\n",
        "\n",
        "**Attention scores from Word 2 (\"really\")'s perspective (using $Q_2$):**\n",
        "* Score of \"really\" attending to \"I\":\n",
        "    $Score(2,1) = Q_2 \\cdot K_1^T = [3.66, 4.12] \\cdot [1.50, 4.00]^T = (3.66 * 1.50) + (4.12 * 4.00) = 5.49 + 16.48 = 21.97$\n",
        "* Score of \"really\" attending to \"really\":\n",
        "    $Score(2,2) = Q_2 \\cdot K_2^T = [3.66, 4.12] \\cdot [4.12, 3.66]^T = (3.66 * 4.12) + (4.12 * 3.66) = 15.0792 + 15.0792 = 30.1584$\n",
        "\n",
        "Softmax for Word 2: $Weights_2 = \\text{Softmax}([21.97, 30.1584])$\n",
        "$e^{21.97} \\approx 3.46 \\times 10^9$\n",
        "$e^{30.1584} \\approx 1.25 \\times 10^{13}$\n",
        "Sum $\\approx 1.25 \\times 10^{13}$\n",
        "$Weight(\\text{really} \\rightarrow I) = (3.46 \\times 10^9) / (1.25 \\times 10^{13}) \\approx 0.00027$\n",
        "$Weight(\\text{really} \\rightarrow \\text{really}) = (1.25 \\times 10^{13}) / (1.25 \\times 10^{13}) \\approx 0.99973$\n",
        "So, for \"really\", it also pays overwhelmingly more attention to itself in this specific setup. (Note: this is just a result of our arbitrary $W_Q, W_K$ values).\n",
        "\n",
        "### 5. How Order is \"Learned\"\n",
        "\n",
        "1.  **Unique Input Vectors:** The crucial starting point is that $X_1$ (for \"I\" at pos 0) and $X_2$ (for \"really\" at pos 1) are unique. The positional encoding ensures this. Even if we had the same word at two different positions, their $X$ vectors would differ due to PE.\n",
        "\n",
        "2.  **Transformation by Learned Weights ($W_Q, W_K$):** The weight matrices $W_Q$ and $W_K$ are **learned** during training. They are adjusted through backpropagation to minimize the model's prediction error on a large corpus of text.\n",
        "    * The model learns to project the $X$ vectors (which contain both semantic and positional information) into $Q$ and $K$ spaces in such a way that their dot products reflect meaningful relationships.\n",
        "    * Because $X_1$ and $X_2$ have different positional \"flavors\" due to PE, $Q_1, K_1$ will be different from $Q_2, K_2$ in ways that reflect this.\n",
        "\n",
        "3.  **Positional Influence on Scores:** The attention scores $Q_i \\cdot K_j^T$ depend directly on the values in $Q_i$ and $K_j$. Since these values were derived from $X_i$ and $X_j$ (which include positional information), the attention scores are sensitive to the original positions of words `i` and `j`.\n",
        "    * For example, the model might learn through $W_Q$ and $W_K$ that a $Q$ vector derived from an $X$ vector with \"early position PE characteristics\" should have a high dot product with a $K$ vector derived from an $X$ vector with \"next position PE characteristics\" if certain semantic features are also present.\n",
        "\n",
        "4.  **Example of Order Dependence:**\n",
        "    Consider two sentences:\n",
        "    A) \"Dog bites man\"\n",
        "    B) \"Man bites dog\"\n",
        "\n",
        "    * The word embedding for \"dog\" is the same in both. The word embedding for \"man\" is the same.\n",
        "    * However:\n",
        "        * In A: `X_dog = E_dog + PE_0`, `X_bites = E_bites + PE_1`, `X_man = E_man + PE_2`\n",
        "        * In B: `X_man = E_man + PE_0`, `X_bites = E_bites + PE_1`, `X_dog = E_dog + PE_2`\n",
        "    * When \"dog\" is the query word in sentence A (i.e., $Q_{dog}$ from $X_{dog}$ in A), its attention to \"bites\" ($K_{bites}$ from $X_{bites}$ in A) and \"man\" ($K_{man}$ from $X_{man}$ in A) will be based on interactions like `(E_dog + PE_0)` with `(E_bites + PE_1)` and `(E_man + PE_2)`.\n",
        "    * When \"dog\" is the query word in sentence B, it's now at the end. Its $Q_{dog}$ is derived from `(E_dog + PE_2)`. Its attention to \"man\" (now `K_man` from `E_man + PE_0`) and \"bites\" (`K_bites` from `E_bites + PE_1`) will be different because the PEs associated with each word (and thus their $Q$ and $K$ vectors) are different relative to \"dog\".\n",
        "\n",
        "    The model learns, through the $W_Q, W_K, W_V$ matrices, to interpret these positionally-informed $Q$ and $K$ vectors. It learns that a noun with $PE_0$ characteristics acting as an agent might look for a verb with $PE_1$ characteristics. This is not a hardcoded rule, but rather a pattern learned from data, enabled by the fact that PEs make the inputs for different positions distinguishable.\n",
        "\n",
        "In essence, the positional encodings provide unique \"coordinates\" or \"signatures\" for each position. The learned weight matrices $W_Q, W_K, W_V$ allow the attention mechanism to become sensitive to these signatures, enabling it to model relationships that depend on the order and relative positions of words. The model doesn't \"read\" the position numbers but learns to associate patterns in the combined (embedding + PE) vectors with typical sequential roles and dependencies."
      ],
      "metadata": {
        "id": "x1C8AS-TFD4r"
      }
    }
  ]
}